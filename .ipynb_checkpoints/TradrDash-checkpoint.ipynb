{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'pandas'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-b4b06d7f0150>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mwarnings\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mnumpy\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m \u001b[0;32mimport\u001b[0m \u001b[0mpandas\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     10\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mast\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mliteral_eval\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtextblob\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mTextBlob\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'pandas'"
     ]
    }
   ],
   "source": [
    "import base64\n",
    "import io\n",
    "import re\n",
    "import ast\n",
    "import csv\n",
    "import nltk\n",
    "import warnings\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from ast import literal_eval\n",
    "from textblob import TextBlob\n",
    "import matplotlib.pyplot as plt\n",
    "from wordcloud import WordCloud\n",
    "from nltk.corpus import stopwords\n",
    "from itertools import zip_longest\n",
    "from nltk import WordNetLemmatizer\n",
    "from nltk.stem import PorterStemmer\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.tokenize import word_tokenize\n",
    "from sklearn.impute import SimpleImputer\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "from gensim.utils import simple_preprocess\n",
    "from sklearn import datasets, linear_model\n",
    "from gensim.test.utils import common_texts\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.datasets import make_classification\n",
    "from sklearn.neighbors import KNeighborsRegressor\n",
    "from sklearn.metrics import classification_report\n",
    "from gensim.parsing.preprocessing import STOPWORDS\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "from gensim.models.doc2vec import Doc2Vec, TaggedDocument\n",
    "from nltk.sentiment.vader import SentimentIntensityAnalyzer\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.linear_model import LinearRegression, Ridge, Lasso, ElasticNet\n",
    "from sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.tree import plot_tree\n",
    "\n",
    "\n",
    "# Run first time\n",
    "nltk.download('wordnet')\n",
    "nltk.download('stopwords')\n",
    "nltk.download('vader_lexicon')\n",
    "\n",
    "# Ignore the warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "#"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'pd' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-3-0601fbb33b14>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# Read in scrapped data\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mdf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_csv\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;34m'./Data/main/ScrappedReddit.csv'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mPriceData\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_csv\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;34m'./Data/PriceData.csv'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'pd' is not defined"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "# Read in scrapped data\n",
    "df = pd.read_csv ('./Data/main/ScrappedReddit.csv')\n",
    "\n",
    "\n",
    "PriceData = pd.read_csv ('./Data/PriceData.csv')\n",
    "\n",
    "# Read in buy prices\n",
    "# buyPrice = pd.read_csv ('BuyPrice.csv')\n",
    "\n",
    "# Parse and merge data\n",
    "PriceData[\"parsedDate\"] = pd.to_datetime(PriceData[\"date\"], infer_datetime_format=True)\n",
    "\n",
    "PriceData[\"dateOnly\"] = PriceData[\"parsedDate\"].dt.date\n",
    "\n",
    "PriceData[\"hourOnly\"] = PriceData[\"parsedDate\"].dt.hour\n",
    "\n",
    "\n",
    "df[\"parsedDate\"] = pd.to_datetime(df[\"Hour\"], infer_datetime_format=True)\n",
    "df[\"dateOnly\"] = df[\"parsedDate\"].dt.date\n",
    "df[\"hourOnly\"] = df[\"parsedDate\"].dt.hour\n",
    "\n",
    "# Merge on dates\n",
    "mergedDf = df.merge(PriceData, how='left', on=['dateOnly', 'hourOnly'])\n",
    "\n",
    "\n",
    "# Drop unused columns\n",
    "mergedDf = mergedDf.drop(columns=['parsedDate_y'])\n",
    "mergedDf = mergedDf.drop(columns=['parsedDate_x'])\n",
    "# mergedDf = mergedDf.drop(columns=['date_x'])\n",
    "# mergedDf = mergedDf.drop(columns=['date_y'])\n",
    "\n",
    "# Merge the data and prices\n",
    "mergedDf['bitcoin_pctChange'] = mergedDf['bitcoin_price'].pct_change().shift(-1)\n",
    "mergedDf['btc_pctChange'] = mergedDf['btc_price'].pct_change().shift(-1)\n",
    "mergedDf['ethereum_pctChange'] = mergedDf['ethereum_price'].pct_change().shift(-1)\n",
    "mergedDf['monero_pctChange'] = mergedDf['monero_price'].pct_change().shift(-1)\n",
    "mergedDf['dashpay_pctChange'] = mergedDf['dashpay_price'].pct_change().shift(-1)\n",
    "mergedDf['ethfinance_pctChange'] = mergedDf['ethereum_price'].pct_change().shift(-1)\n",
    "mergedDf['ethtrader_pctChange'] = mergedDf['ethereum_price'].pct_change().shift(-1)\n",
    "mergedDf['xmrtrader_pctChange'] = mergedDf['monero_price'].pct_change().shift(-1)\n",
    "\n",
    "\n",
    "# Copy prices for trader subs\n",
    "mergedDf['xmrtrader_price'] = mergedDf['monero_price']\n",
    "mergedDf['ethfinance_price'] = mergedDf['ethereum_price']\n",
    "mergedDf['ethtrader_price'] = mergedDf['ethereum_price']\n",
    "\n",
    "\n",
    "mergedDf['bitcoin_signal'] = (mergedDf['bitcoin_pctChange'] > 0).astype(int)\n",
    "mergedDf['btc_signal'] = (mergedDf['btc_pctChange'] > 0).astype(int)\n",
    "mergedDf['ethereum_signal'] = (mergedDf['ethereum_pctChange'] > 0).astype(int)\n",
    "mergedDf['monero_signal'] = (mergedDf['monero_pctChange'] > 0).astype(int)\n",
    "mergedDf['dashpay_signal'] = (mergedDf['dashpay_pctChange'] > 0).astype(int)\n",
    "mergedDf['ethfinance_signal'] = (mergedDf['ethfinance_pctChange'] > 0).astype(int)\n",
    "mergedDf['ethtrader_signal'] = (mergedDf['ethtrader_pctChange'] > 0).astype(int)\n",
    "mergedDf['xmrtrader_signal'] = (mergedDf['xmrtrader_pctChange'] > 0).astype(int)\n",
    "\n",
    "# Rename and drop unused columns\n",
    "df = mergedDf.copy()\n",
    "df = df.drop(columns=['dateOnly', 'hourOnly'])\n",
    "\n",
    "\n",
    "# Function for extracting the comments\n",
    "def extract_comment(df, coinComments):\n",
    "    comment_post = []  # Opening empty list\n",
    "\n",
    "    for i in range(len(df)):\n",
    "        dictTest = df[coinComments][i]\n",
    "\n",
    "        try:\n",
    "            dictTest = ast.literal_eval(dictTest)\n",
    "            final_comment = \"\"\n",
    "\n",
    "            for element in dictTest:\n",
    "                if element['comments'] != '0':\n",
    "                    for comment in element['comments_on_post']:\n",
    "                        final_comment += comment + \" \"\n",
    "            if final_comment != \"\":\n",
    "                comment_post.append(final_comment)\n",
    "            else:\n",
    "                comment_post.append(np.nan)\n",
    "        except:\n",
    "            comment_post.append(np.nan)\n",
    "    return comment_post\n",
    "\n",
    "\n",
    "# Passing  the dataframes into the function\n",
    "btcCommentsBlob = extract_comment(df, 'btc_comments')\n",
    "bitcoinCommentsBlob = extract_comment(df, 'bitcoin_comments')\n",
    "ethereumCommentsBlob = extract_comment(df, 'ethereum_comments')\n",
    "moneroCommentsBlob = extract_comment(df, 'monero_comments')\n",
    "dashpayCommentsBlob = extract_comment(df, 'dashpay_comments')\n",
    "ethtraderCommentsBlob = extract_comment(df, 'ethtrader_comments')\n",
    "ethfinanceCommentsBlob = extract_comment(df, 'ethfinance_comments')\n",
    "xmrtraderCommentsBlob = extract_comment(df, 'xmrtrader_comments')\n",
    "\n",
    "# Converting into dataframe\n",
    "dfComments = pd.DataFrame({'btc_comments': btcCommentsBlob, 'bitcoin_comments': bitcoinCommentsBlob,\n",
    "                           'ethereum_comments': ethereumCommentsBlob, 'monero_comments': moneroCommentsBlob,\n",
    "                           'dashpay_comments': dashpayCommentsBlob, 'ethtrader_comments': ethtraderCommentsBlob,\n",
    "                           'ethfinance_comments': ethfinanceCommentsBlob, 'xmrtrader_comments': xmrtraderCommentsBlob})\n",
    "\n",
    "\n",
    "# Clean up the comments\n",
    "\n",
    "def _removeNonAscii(s):\n",
    "    return \"\".join(i for i in s if ord(i) < 128)\n",
    "\n",
    "\n",
    "def make_lower_case(text):\n",
    "    return text.lower()\n",
    "\n",
    "\n",
    "def remove_html(text):\n",
    "    html_pattern = re.compile('<.*?>')\n",
    "    return html_pattern.sub(r'', text)\n",
    "\n",
    "\n",
    "def remove_punctuation(text):\n",
    "    tokenizer = RegexpTokenizer(r'\\w+')\n",
    "    text = tokenizer.tokenize(text)\n",
    "    text = \" \".join(text)\n",
    "    return text\n",
    "\n",
    "\n",
    "def clean(x):\n",
    "    x = re.sub(r'^RT[\\s]+', '', x)\n",
    "    x = re.sub(r'https?:\\/\\/.*[\\r\\n]*', '', x)\n",
    "    x = re.sub(r'#', '', x)\n",
    "    x = re.sub(r'@[A-Za-z0–9]+', '', x)\n",
    "    return x\n",
    "\n",
    "\n",
    "def lemmatize_stemming(text):\n",
    "    return ''.join(lemtzer.lemmatize(text, pos='v'))\n",
    "\n",
    "\n",
    "def preprocess(text):\n",
    "    result = []\n",
    "    for token in simple_preprocess(text):\n",
    "        if token not in STOPWORDS:\n",
    "            result.append(lemmatize_stemming(token))\n",
    "\n",
    "    return ' '.join(result)\n",
    "\n",
    "\n",
    "def cleanComments(coinColName):\n",
    "    dfComments[coinColName] = dfComments[coinColName].astype(str).apply(_removeNonAscii)\n",
    "    dfComments[coinColName] = dfComments[coinColName].apply(func=make_lower_case)\n",
    "    dfComments[coinColName] = dfComments[coinColName].apply(func=remove_punctuation)\n",
    "    dfComments[coinColName] = dfComments[coinColName].apply(func=remove_html)\n",
    "    dfComments[coinColName] = dfComments[coinColName].apply(func=clean)\n",
    "    dfComments[coinColName] = dfComments[coinColName].apply(func=lemmatize_stemming)\n",
    "    dfComments[coinColName] = dfComments[coinColName].apply(func=preprocess)\n",
    "\n",
    "\n",
    "stop_words = set(stopwords.words('english'))\n",
    "lemtzer = WordNetLemmatizer()\n",
    "\n",
    "colList = [\"btc_comments\", \"bitcoin_comments\", \"ethereum_comments\", \"monero_comments\", \"dashpay_comments\",\n",
    "           \"ethtrader_comments\", \"ethfinance_comments\", \"xmrtrader_comments\"]\n",
    "\n",
    "for coin in colList:\n",
    "    cleanComments(coin)\n",
    "\n",
    "\n",
    "\n",
    "#Function for polarity and subjectivity\n",
    "polarity = lambda x: TextBlob(x).sentiment.polarity\n",
    "subjectivity = lambda x: TextBlob(x).sentiment.subjectivity\n",
    "\n",
    "#Passing the function to calculate polarity and subjectivity\n",
    "dfComments['btc_polarity'] = dfComments['btc_comments'].apply(polarity)\n",
    "dfComments['btc_subjectivity'] = dfComments['btc_comments'].apply(subjectivity)\n",
    "dfComments['bitcoin_polarity'] = dfComments['bitcoin_comments'].apply(polarity)\n",
    "dfComments['bitcoin_subjectivity'] = dfComments['bitcoin_comments'].apply(subjectivity)\n",
    "dfComments['ethereum_polarity'] = dfComments['ethereum_comments'].apply(polarity)\n",
    "dfComments['ethereum_subjectivity'] = dfComments['ethereum_comments'].apply(subjectivity)\n",
    "dfComments['monero_polarity'] = dfComments['monero_comments'].apply(polarity)\n",
    "dfComments['monero_subjectivity'] = dfComments['monero_comments'].apply(subjectivity)\n",
    "dfComments['dashpay_polarity'] = dfComments['dashpay_comments'].apply(polarity)\n",
    "dfComments['dashpay_subjectivity'] = dfComments['dashpay_comments'].apply(subjectivity)\n",
    "dfComments['ethtrader_polarity'] = dfComments['ethtrader_comments'].apply(polarity)\n",
    "dfComments['ethtrader_subjectivity'] = dfComments['ethtrader_comments'].apply(subjectivity)\n",
    "dfComments['ethfinance_polarity'] = dfComments['ethfinance_comments'].apply(polarity)\n",
    "dfComments['ethfinance_subjectivity'] = dfComments['ethfinance_comments'].apply(subjectivity)\n",
    "dfComments['xmrtrader_polarity'] = dfComments['xmrtrader_comments'].apply(polarity)\n",
    "dfComments['xmrtrader_subjectivity'] = dfComments['xmrtrader_comments'].apply(subjectivity)\n",
    "\n",
    "# Rename columns\n",
    "df['btc_polarity'] = dfComments['btc_polarity']\n",
    "df['btc_subjectivity'] = dfComments['btc_subjectivity']\n",
    "df['bitcoin_polarity'] = dfComments['bitcoin_polarity']\n",
    "df['bitcoin_subjectivity'] = dfComments['bitcoin_subjectivity']\n",
    "df['ethereum_polarity'] = dfComments['ethereum_polarity']\n",
    "df['ethereum_subjectivity'] =dfComments['ethereum_subjectivity']\n",
    "df['monero_polarity'] = dfComments['monero_polarity']\n",
    "df['monero_subjectivity'] =dfComments['monero_subjectivity']\n",
    "df['dashpay_polarity'] = dfComments['dashpay_polarity']\n",
    "df['dashpay_subjectivity'] = dfComments['dashpay_subjectivity']\n",
    "df['ethtrader_polarity'] = dfComments['ethtrader_polarity']\n",
    "df['ethtrader_subjectivity'] = dfComments['ethtrader_subjectivity']\n",
    "df['ethfinance_polarity'] = dfComments['ethfinance_polarity']\n",
    "df['ethfinance_subjectivity'] = dfComments['ethfinance_subjectivity']\n",
    "df['xmrtrader_polarity'] = dfComments['xmrtrader_polarity']\n",
    "df['xmrtrader_subjectivity'] = dfComments['xmrtrader_subjectivity']\n",
    "\n",
    "df['btc_comments'] = dfComments['btc_comments']\n",
    "df['bitcoin_comments'] = dfComments['bitcoin_comments']\n",
    "df['ethereum_comments'] = dfComments['ethereum_comments']\n",
    "df['ethtrader_comments'] = dfComments['ethtrader_comments']\n",
    "df['ethfinance_comments'] = dfComments['ethfinance_comments']\n",
    "df['dashpay_comments'] = dfComments['dashpay_comments']\n",
    "df['monero_comments'] = dfComments['monero_comments']\n",
    "df['xmrtrader_comments'] = dfComments['xmrtrader_comments']\n",
    "\n",
    "\n",
    "df[\"btc_symbol\"] = \"BTC\"\n",
    "df[\"ethereum_symbol\"] = \"ETH\"\n",
    "df[\"ethfinance_symbol\"] = \"ETH\"\n",
    "df[\"ethtrader_symbol\"] = \"ETH\"\n",
    "df[\"bitcoin_symbol\"] = \"BITCOIN\"\n",
    "df[\"dashpay_symbol\"] = \"DASH\"\n",
    "df[\"monero_symbol\"] = \"XMR\"\n",
    "df[\"xmrtrader_symbol\"] = \"XMR\"\n",
    "\n",
    "\n",
    "df[\"ethtrader_volume\"] = df[\"ethereum_volume\"]\n",
    "df[\"ethfinance_volume\"] = df[\"ethereum_volume\"]\n",
    "df[\"xmrtrader_volume\"] = df[\"monero_volume\"]\n",
    "\n",
    "# Clean (tidy) the data\n",
    "\n",
    "tidyData = pd.DataFrame(\n",
    "    columns=[\"hour\", \"online_users\", \"number_of_post\", \"comments\", \"total_votes\", \"polarity\", \"subjectivity\", \"price\",\n",
    "             \"signal\", \"pctChange\", \"volume\", 'symbol'])\n",
    "subRedditName = ['bitcoin', 'btc', 'ethereum', 'monero', 'dashpay', 'ethtrader', 'ethfinance', 'xmrtrader']\n",
    "\n",
    "for subName in subRedditName:\n",
    "    sufVals = [\"online_users\", \"number_of_post\", \"comments\", \"total_votes\", \"polarity\", \"subjectivity\", \"price\",\n",
    "               \"signal\", \"pctChange\", \"volume\", 'symbol']\n",
    "    chunk = df[[\"Hour\"] + [subName + \"_{}\".format(i) for i in sufVals]]\n",
    "    chunk.columns = ['hour'] + sufVals\n",
    "    chunk['subReddit'] = subName\n",
    "    tidyData = pd.concat([tidyData, chunk])\n",
    "\n",
    "tidyData = pd.DataFrame(tidyData)\n",
    "\n",
    "# More NLP to do 2nd sentiment analysis and make doc2vec (did not work very well, dont use)\n",
    "\n",
    "newComments = tidyData\n",
    "\n",
    "sid = SentimentIntensityAnalyzer()\n",
    "newComments[\"sentiments\"] = tidyData[\"comments\"].apply(lambda x: sid.polarity_scores(x))\n",
    "newComments = pd.concat([newComments.drop(['sentiments'], axis=1), newComments['sentiments'].apply(pd.Series)], axis=1)\n",
    "\n",
    "tidyData = newComments\n",
    "newNLP = tidyData.comments\n",
    "\n",
    "documents = [TaggedDocument(doc, [i]) for i, doc in enumerate(newNLP)]\n",
    "# try differnt vector sizes!\n",
    "model = Doc2Vec(documents, vector_size=50, window=5, min_count=1, workers=4)\n",
    "\n",
    "# model.docvecs[0]\n",
    "newNLP2 = pd.DataFrame([model.docvecs[i] for i in range(len(newNLP))])\n",
    "# newNLP2\n",
    "\n",
    "# Clean data more, number of active user numbners (remove \"k\"), fix data time format, remove nan\n",
    "\n",
    "def removeKs(dfNum):\n",
    "    dfNum = str(dfNum).lstrip('0')\n",
    "    dfNum = str(dfNum).lstrip()\n",
    "    if len(dfNum) == 0:\n",
    "        return 0\n",
    "\n",
    "    if dfNum[-1] == 'k':\n",
    "        return int(float(dfNum[0:-1]) * 1000)\n",
    "\n",
    "    return int(float(dfNum))\n",
    "\n",
    "\n",
    "tidyData[\"online_users\"] = tidyData[\"online_users\"].map(lambda x: removeKs(x))\n",
    "\n",
    "\n",
    "# Date/time format and generate day of week and hour of day feature\n",
    "tidyData = tidyData.reset_index(drop=True)\n",
    "tidyData['hour'] = pd.to_datetime(tidyData['hour'])\n",
    "\n",
    "\n",
    "tidyData['hour'] = pd.to_datetime(tidyData['hour'], format = '%m/%d/%y %H:%M %p')\n",
    "\n",
    "tidyData['day_of_week'] = tidyData.hour.dt.dayofweek\n",
    "tidyData['hour_of_day'] = tidyData.hour.dt.hour\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Remove Na's\n",
    "tidyData['number_of_post'] = tidyData['number_of_post'].fillna(0)\n",
    "tidyData['total_votes'] = tidyData['total_votes'].fillna(0)\n",
    "tidyData['pctChange'] = tidyData['pctChange'].fillna(0)\n",
    "\n",
    "\n",
    "# Dont need:\n",
    "# target = tidyData.pctChange\n",
    "# features = tidyData.copy().drop(columns =[\"hour\", \"signal\", \"comments\", \"symbol\", 'pctChange'], axis=1)\n",
    "# dummyFeatures = pd.get_dummies(features)\n",
    "\n",
    "\n",
    "# Generate list of most important words\n",
    "\n",
    "commentsSignal = tidyData.comments\n",
    "count_vec = CountVectorizer()\n",
    "X_train_count = count_vec.fit_transform(commentsSignal)\n",
    "X_train_count.shape\n",
    "X_train_count\n",
    "\n",
    "clf = DecisionTreeClassifier(random_state=0)\n",
    "commnetModel = clf.fit(X_train_count, tidyData.signal.astype(int))\n",
    "\n",
    "### Make a list of important words and score them then count them\n",
    "vectWords = pd.DataFrame(count_vec.get_feature_names())\n",
    "wordScores = pd.DataFrame(commnetModel.feature_importances_)\n",
    "\n",
    "vectWords.columns=['word']\n",
    "wordScores.columns=['score']\n",
    "\n",
    "wordsAndScores = wordScores.join(vectWords)\n",
    "\n",
    "dropRows = wordsAndScores[wordsAndScores['score'] == 0].index\n",
    "\n",
    "\n",
    "# Delete these row indexes from dataFrame and make a word list\n",
    "wordsAndScores.drop(dropRows , inplace=True)\n",
    "\n",
    "wordScores = wordsAndScores.sort_values('score', ascending = False).iloc[:15]\n",
    "\n",
    "wordsOnly = wordScores.drop(columns='score')\n",
    "\n",
    "wordsOnly.reset_index(drop=True, inplace=True)\n",
    "wordsOnlyT = wordsOnly.T\n",
    "wordList = wordsOnly.word.tolist()\n",
    "\n",
    "headRow=1\n",
    "wordsOnlyT.columns = wordsOnlyT.iloc[0]\n",
    "wordsOnlyT.reset_index(drop=True, inplace=True)\n",
    "\n",
    "wideWithWords = pd.concat([tidyData.comments, wordsOnlyT])\n",
    "wideWithWords.rename(columns={ wideWithWords.columns[0]: \"comments\" }, inplace = True)\n",
    "# wideWithWords.to_csv('HowToCountWords.csv')\n",
    "\n",
    "# Make the counts and set new features!!! (Only important part)\n",
    "g=pd.get_dummies(pd.Series(wideWithWords.comments.str.split('\\s').explode())).reindex(columns=wordList).fillna(0).astype(int)\n",
    "wordCounts = pd.DataFrame(wideWithWords.iloc[:,0]).join(g.groupby(level=0).sum(0)).drop(columns='comments')\n",
    "\n",
    "wordCounts = wordCounts[:-1]\n",
    "wordCounts = wordCounts.reset_index(drop=True)\n",
    "\n",
    "outputFive = wordCounts.sum()\n",
    "outputFive = outputFive.sort_values(ascending=False)\n",
    "\n",
    "\n",
    "##################################\n",
    "#####   SET FEATURES HERE   ######\n",
    "\n",
    "\n",
    "# HERE IS WHERE WE INPUT THE USER'S SELECTED FEATURES\n",
    "\n",
    "# FEATURES to choose from:\n",
    "#  ['hour_of_day', 'day_of_week', 'online_users', 'number_of_post', 'total_votes', 'polarity',\n",
    "#   'subjectivity', 'volume', 'negativity', 'neutrality', 'positivty', 'top_10_words']\n",
    "\n",
    "\n",
    "############################\n",
    "### FIRST SET OF OUTPUTS ###\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# print(classification_report(signalTest, testResults, target_names=target_names))\n",
    "# print(f'r^2 Test is {rf_model.score(featuresTest, signalTest)}')\n",
    "\n",
    "\n",
    "#\n",
    "\n",
    "#\n",
    "import dash\n",
    "import dash_core_components as dcc\n",
    "import dash_html_components as html\n",
    "import dash_bootstrap_components as dbc\n",
    "from dash.dependencies import Input,Output,State\n",
    "from dash_table import DataTable\n",
    "import plotly.express as px\n",
    "\n",
    "app=dash.Dash(__name__,external_stylesheets=[dbc.themes.BOOTSTRAP],meta_tags=[{'name': 'viewport','content': 'width=device-width, initial-scale=1.0'}])\n",
    "\n",
    "\n",
    "app.layout=dbc.Container([\n",
    "    dbc.Row([\n",
    "        dbc.Col('Bitcoin Prediction',className=\"text-center text-danger\")\n",
    "    ]),\n",
    "    dbc.Row([\n",
    "        dbc.Col(html.H1('Assests')),\n",
    "        dbc.Col(html.H1('Features To Drop')),\n",
    "        ]\n",
    "    ),\n",
    "    dbc.Row(\n",
    "        dbc.Col([\n",
    "            dcc.Dropdown(id='features', options=[{'label': 'hour_of_day', 'value': 'hour_of_day'},\n",
    "                                                 {'label': 'day_of_week', 'value': 'day_of_week'},\n",
    "                                                 {'label': 'online_users', 'value': 'online_users'},\n",
    "                                                 {'label': 'number_of_post', 'value': 'number_of_post'},\n",
    "                                                 {'label': 'total_votes', 'value': 'total_votes'},\n",
    "                                                 {'label': 'polarity', 'value': 'polarity'},\n",
    "                                                 {'label': 'subjectivity', 'value': 'subjectivity'},\n",
    "                                                 {'label': 'negativity', 'value': 'neg'},\n",
    "                                                 {'label': 'neutrality', 'value': 'neu'},\n",
    "                                                 {'label': 'positivty', 'value': 'pos'},\n",
    "                                                 {'label': 'top_10_words', 'value': 'top_10_words'}], multi=True,value=['price'])\n",
    "        ]),\n",
    "    ),\n",
    "    dbc.Row(\n",
    "        dbc.Col(html.Button('Predict',id='predict',n_clicks=0))\n",
    "    ),\n",
    "    dbc.Row([\n",
    "        dbc.Col(\n",
    "            dbc.RadioItems(id='assets',options=[{'label':'BTC','value':'symbol_BTC'},\n",
    "                                           {'label':'BITCOIN','value':'symbol_BITCOIN'},\n",
    "                                           {'label':'DASH','value':'symbol_DASH'},\n",
    "                                           {'label':'ETH','value':'symbol_ETH'},\n",
    "                                           {'label':'XMR','value':'symbol_XMR'}])),\n",
    "            dbc.Col(\n",
    "                DataTable(id='output1')\n",
    "            ),\n",
    "        dbc.Col(\n",
    "            DataTable(id='output0')\n",
    "        ),\n",
    "            dbc.Col(\n",
    "                html.Div(id='output2')\n",
    "            )]),\n",
    "        dbc.Row(\n",
    "            dbc.Col(\n",
    "                DataTable(id='output3')\n",
    "            )\n",
    "        ),\n",
    "        dbc.Row([\n",
    "            dbc.Col(\n",
    "                dcc.Graph(id='output4')\n",
    "            ),\n",
    "            dbc.Col(\n",
    "                dcc.Graph(figure=px.bar(x=outputFive.index[:-1],y=outputFive.values[:-1]))\n",
    "            ),\n",
    "        ]),\n",
    "    dbc.Row(\n",
    "            html.Img(id='example')\n",
    "    )\n",
    "\n",
    "],fluid=True)\n",
    "\n",
    "\n",
    "lists=['comments','pctChange','volume']\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "@app.callback(Output('output1','data'),Output('output1','columns'),Output('output0','data'),Output('output0','columns'),Output('output2','children'),Output('output3','data'),Output('output3','columns'),Output('output4','figure'),Output('example', 'src'),Input('predict','n_clicks'),State('features','value'))\n",
    "def update_features(predicts,featuress):\n",
    "\n",
    "    ##################################\n",
    "    #####   SET FEATURES HERE   ######\n",
    "\n",
    "    # HERE IS WHERE WE INPUT THE USER'S SELECTED FEATURES\n",
    "\n",
    "    # FEATURES to choose from:\n",
    "    #  ['hour_of_day', 'day_of_week', 'online_users', 'number_of_post', 'total_votes', 'polarity',\n",
    "    #   'subjectivity', 'volume', 'negativity', 'neutrality', 'positivty', 'top_10_words']\n",
    "    tidyDataa = pd.concat([tidyData, wordCounts], axis=1)\n",
    "    print(tidyDataa.columns)\n",
    "    for i in featuress:\n",
    "        lists.append(i)\n",
    "    print(lists)\n",
    "    if predicts:\n",
    "        features = tidyDataa.copy().drop(columns=lists, axis=1)\n",
    "        print(features)\n",
    "        dummyFeatures = pd.get_dummies(features)\n",
    "        # features = pd.concat([features, newNLP2], axis = 1)\n",
    "\n",
    "        # dummyFeatures.to_csv('afterdumbdumb.csv')\n",
    "\n",
    "        ### test day of week\n",
    "\n",
    "        # dummyDays = pd.get_dummies(dummyFeatures['day_of_week'])\n",
    "        # dummyFeatures = dummyFeatures.drop(columns =[\"day_of_week\"], axis=1)\n",
    "        # dummyFeatures = pd.concat([dummyFeatures, dummyDays], axis = 1)\n",
    "        if 'hour_of_day' in dummyFeatures.columns:\n",
    "            dummyHours = pd.get_dummies(dummyFeatures['hour_of_day'])\n",
    "            dummyFeatures = dummyFeatures.drop(columns=[\"hour_of_day\"], axis=1)\n",
    "            dummyFeatures = pd.concat([dummyFeatures, dummyHours], axis=1)\n",
    "        else:\n",
    "            pass\n",
    "\n",
    "        # features.to_csv('beforedumb.csv')\n",
    "\n",
    "        dummyFeatures = dummyFeatures.sort_values('hour')\n",
    "\n",
    "        target = dummyFeatures.signal_1.astype(int)\n",
    "\n",
    "        dummyFeatures = dummyFeatures.drop(columns=['hour', 'signal_1', 'signal_0'], axis=1)\n",
    "\n",
    "\n",
    "        newRowToPredict = dummyFeatures.iloc[-8:]\n",
    "        dummyFeatures = dummyFeatures.iloc[:-8]\n",
    "        targetToPredict = target.iloc[-8:]\n",
    "        target = target.iloc[:-8]\n",
    "\n",
    "        featuresTrain, featuresTest, signalTrain, signalTest = train_test_split(dummyFeatures, target,test_size=0.2,random_state=42)\n",
    "        rf_model = RandomForestClassifier()\n",
    "        param_grid = {'max_depth': np.arange(5, 40, 5), 'min_samples_split': np.arange(2, 10, 2)}\n",
    "        gsModelTrain = GridSearchCV(estimator=rf_model, param_grid=param_grid, cv=3, scoring='roc_auc')\n",
    "        gsModelTrain.fit(featuresTrain, signalTrain)\n",
    "        rf_model.set_params(**gsModelTrain.best_params_)\n",
    "        rf_model.fit(featuresTrain, signalTrain)\n",
    "        plot_tree(rf_model.estimators_[0],filled=True,max_depth=5)\n",
    "        buf = io.BytesIO()\n",
    "        plt.savefig(buf, format=\"png\")\n",
    "        data = base64.b64encode(buf.getbuffer()).decode(\"utf8\")\n",
    "        plt.close()\n",
    "\n",
    "        testResults = rf_model.predict(featuresTest)\n",
    "        target_names = ['class 0', 'class 1']\n",
    "        outputOne = classification_report(signalTest, testResults, target_names=target_names,output_dict=True)\n",
    "        df_report = pd.DataFrame(outputOne)\n",
    "        df_reporttranspose=pd.DataFrame(outputOne).transpose()\n",
    "        reports_columns = [{\"name\": i, \"id\": i} for i in df_report.columns]\n",
    "        reporttranspose_columns=[{\"name\": i, \"id\": i} for i in df_reporttranspose.columns]\n",
    "        df_reporttranspose=df_reporttranspose.to_dict('records')\n",
    "        df_report=df_report.to_dict('records')\n",
    "        outputTwo = \"Score: \"+str(rf_model.score(featuresTest, signalTest))\n",
    "        d1 = pd.DataFrame(rf_model.predict(newRowToPredict.values.reshape(8, -1)), columns=['Predictions'])\n",
    "        d2 = pd.DataFrame(['Monero', 'EthTrader', 'Dash', 'Ethereum', 'BCH', 'XMRTrader', 'Bitcoin', 'EthFinance'],columns=['Assests'])\n",
    "        outputThree = pd.concat([d1, d2], axis=1)\n",
    "        assets_column=[{\"name\": i, \"id\": i} for i in outputThree.columns]\n",
    "        outputThree=outputThree.to_dict('records')\n",
    "        outputFour =pd.Series(rf_model.feature_importances_, index=dummyFeatures.columns).head(10)\n",
    "        outputFour=px.bar(data_frame=outputFour,x=outputFour.index,y=outputFour.values)\n",
    "        return df_report,reports_columns,df_reporttranspose,reporttranspose_columns,outputTwo,outputThree,assets_column,outputFour,\"data:image/png;base64,{}\".format(data)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "if __name__=='__main__':\n",
    "    app.run_server(debug=False)\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
